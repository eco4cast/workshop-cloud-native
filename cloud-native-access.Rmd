---
title: "cloud native access"
output: github_document
date: '2022-05-05'
---


<--! some of the intro discussion of how this works might benefit from a couple
slides/images of your computer/local & cloud w/ notes on where stuff happens -->

# Synopsis

In this tutorial we see how to use the cloud native filesystem functions `s3_bucket()` + `open_dataset()` from the `arrow` library to extract subsets and summaries of remote data files. 
If you have used `dplyr` to access remote [relational database connections](https://db.rstudio.com), arrow's `open_dataset()` works a lot like that. 
While the strategy is much the same, `arrow` works directly against 'static' files, there's no need for a database server.
See the README.md for a conceptual introduction.


```{r setup, message=FALSE}
library(tidyverse)
library(lubridate)
library(arrow) # see https://arrow.apache.org/ for more <- I know it's lower down but this would be good to add for folks reading the code off your screen

Sys.setenv(CURLOPT_ACCEPTTIMEOUT_MS=10000) # Jetstream can be laggy under load
```


```{r eg}
# might put an example directory tree with a fake csv file so it feels more real
# when you demonstrate navigating with the Rstudio popup tabs
```

All big commercial cloud providers (e.g. AWS, Google, Azure) provide object stores. Amazon's AWS S3 platform is among the most widely used, and hosts a [large registry of open datasets](https://registry.opendata.aws/), including archives of NOAA global circulation models.  A wide range of software tools have been built around the AWS S3 interface, which has led to the adoption of S3-compatible interfaces by other cloud providers (Google) and open source platforms (CEPH, MINIO). While not an official community standard like POSIX, this makes S3 something of a de-facto standard.

EFI uses the [Jetstream2](https://jetstream-cloud.org/) Cloud's Object Store, maintained by NSF XSEDE program. Unlike commercial providers, Jetstream2 uses an open source system called [CEPH](https://docs.ceph.com/en/quincy/) (developed by RedHat), which supports the S3 interface.
Consequently, we have to override the default "endpoint" (which assumes it will be talking to AWS), to tell it to talk to our server instead.  
The official Jetstream endpoint for the S3 service is `js2.jetstream-cloud.org:8001`, however,
EFI uses a simple reverse-proxy server to provide a custom endpoint to Jetstream2, `data.ecoforecast.org`.  

Cloud-based object stores use credentials to control who has permission to read and write data to the store.  EFI sets most of it's buckets to public-download, so the data can be read without any credentials.
Writing data requires appropriate credentials, with the exception of the submissions bucket, which permits anonymous uploads. 


Here we'll explore a few popular libraries that support such 'cloud-native' data access. First, let's take a look at the [Apache Arrow](https://arrow.apache.org/) framework. Note that though these examples use R, {arrow} clients are available for most major languages, including python, C++, javascript, Go, Rust, etc. 


To access data on the object store, we establish a connection to a desired bucket on the server. (Note that bucket names are global to S3 object stores, and thus must be unique.) The most important EFI buckets are:

- `scores` Contains scores of every forecast submitted, along with summary statistics of the forecasts.
- `forecasts` Raw forecast files submitted to the challenge and validated by the submissions-checker
- `targets` Processed NEON data into the format of variables used in the challenge
- `drivers` Additional possible driver data, such as NOAA GEFS forecasts downscaled to NEON sites.

Here we make a connection to the the `scores` bucket. Note the endpoint override. The `anonymous=TRUE` is optional, but make sure you do not have any AWS credentials set up in environmental variables or in your `~/.aws` directory. These probably correspond to your account on other S3 platforms and will not be compatible with Jetstream2. See section on troubleshooting below. 


```{r}
s3 <- s3_bucket("scores", endpoint_override = "data.ecoforecast.org", anonymous=TRUE)
```

This `s3` connection object has methods that let us interact with the data over this connection.  For instance, we can list contents in this directory (see `?arrow::S3FileSystem` or [online vignette](https://arrow.apache.org/docs/r/articles/fs.html) for details.)


```{r}
s3$ls()
```

Here we see only one directory (folder). Let's look inside that:

```{r}
s3$ls("parquet") # you might want to introduce what parquet is, but more
# briefly - a modern version of csv that says what types the columns are and
# is broken into chunks ('shards') to allow smarter subsetting
# showing them the structure will help even more

s3$ls(recursive=TRUE) |> head() # get all sub-folders but show only first few

```
Note that `ls()` returns the complete path (`parquet/aquatics`, not `aquatics`) to each element.


While we could read in individual files, `arrow` is particularly powerful when all these files share the same _schema_ (exactly the same column names & data types). 
In such cases, we can establish a connection the data across all files at once, _without_ reading all that data in.
Observe how the files are organized into sub-folders by `target_id` (aquatics, phenology, etc) and year.
We can tell `arrow` about this "partitioning", allowing fast data filtering that need not read many of those files.


```{r}
path <- s3$path("parquet")
# some of the details about what a subtree filesystem is and how it could be on
# your computer seemed a little too detailed and potentially confusing
# maybe just say 'path' is a pointer/handle to what you want
ds <- open_dataset(path, partitioning = c("target_id", "year"))
```

Note that opening the dataset requires a `path()` to the remote object, which we get with the `$path()` method, not the `$ls()` method (the latter merely returns text strings that don't contain the actual connection details.) 

**Pro tip***: We could in fact have short-cut this by including the path in our original call to `s3_bucket()`, which is happy to point at any sub-directory of the bucket as well: 

```{r}
s3 <- s3_bucket("scores/parquet", endpoint_override = "data.ecoforecast.org", anonymous=TRUE)
ds <- open_dataset(s3, partitioning = c("target_id", "year"))
```

Or if we don't need stuff like `$ls()` to list bucket contents, we can use the URI-notation for a one-line access:

```{r}
ds <- open_dataset("s3://scores/parquet?endpoint_override=data.ecoforecast.org", partitioning = c("target_id", "year"))
```


The resulting `ds` object is a remote connection to a data.frame-like object. 
A concept here is that this is a _lazy_ connection - it hasn't read in any data into R yet.
If we print the object, it shows us the schema: column names and types, following optional and required columns from the EFI metadata standard:

```{r}
ds
```
Note we have a FileSystemDataset object made up of lots of individual Parquet files!
In addition to the standard fields in an EFI forecast, (`time`, `site_id`, `variable`, etc) this data includes two Strictly Proper Scoring metrics: `crps` and `logs`. 
We also have the corresponding `observed` value (when avialable) that was used to compute these scores.
While many EFI forecasts are submitted as _ensembles_, the scoring files summarize that uncertainty in terms of mean, standard deviation, and quantiles.  (Note that the scoring is still based directly on ensemble methods, and not merely on these summary statistics.)


Viewing the schema instead of the top of the data frame may seem strange at first, but actually is quite handy. This helps us think about what queries we want first.
Being _lazy_ about reading lets our code be faster and smarter -- if we filter the data, it can skip reading in any fields that would have been left out. 
All `select()` and most common `filter()` operations will work with this remote table:

```{r}
pheno <-
  ds |> 
  filter(target_id == "phenology", 
         year == 2022,
         start_time > as_datetime("2022-04-01")) |>
  collect()

pheno
```

Note that some more complex filters, such as those which use regex patterns (see `?grepl`) or other R functions inside the call will not work.
Arrow is also stricter about some conventions -- e.g. experienced users might note we had to explicitly convert our date string to a datetime object in order to use a logical `>` comparison; while dplyr does this automatically with in-memory data.frames.
After our filter, we use the function `collect()` to signal we are ready to read our data into R.
This lets `arrow` know it can be lazy no longer, so expect this step to be slow try and defer this operation as long as possible.


In addition to `select()` and `filter()`, arrow is now capable of executing a wide range of `group_by()` and `summarize()` operations. 
Here's an example of some summary statistics we can compute directly from the data source.
Some of these can be slow to compute depending on your network speed, but all can run without ever downloading the full dataset or ever reading it all into memory. 


```{r}
ds |> distinct(variable) |> collect() |> nrow()
ds |> count(variable, model_id, sort=TRUE) |> collect()
ds |> count(target_id, model_id, start_time) |> collect()
ds |> group_by(variable, model_id) |>  summarise(mean_crps = mean(crps)) |> collect()
ds |> summarise(duration = max(time) - min(time)) |> pull(duration) |> as.numeric(units="days")
```

## Alternative data formats

In this example, the individual files in this data set are in the parquet format, but `open_dataset()` may also be used with tabular data in plain-text formats such as `csv`.
`open_dataset()` can access a single file or many `csv`'s, including those partitioned into sub-directories like the parquet files shown here. 
When reading in csv files, care must be taken to ensure that data types for each column are completely consistent by specifying the schema explicitly.
While `arrow` can attempt to guess if a value is an integer, date, time, or string (etc), these guesses can fail (particularly with missing data or complex types, like difftimes). 
With parquet, this type information is encoded into the data, and the data is also compressed by default.
Parquet is now a widely used open standard for tabular data and may be considered a suitable alternative to `csv` or `hdf5` for serializing tabular data. 


<!-- 
For instance, we can read in a subsection of EFI's target data, which publishes the full date range as a single csv file (unfortunately handling compression is somewhat cumbersome at the moment):

```{r}
target <- s3_bucket("targets/aquatics",
                    endpoint_override="data.ecoforecast.org", anonymous=TRUE)
input <- CompressedInputStream$create(target$OpenInputFile("aquatics-targets.csv.gz"))

trgt <- arrow::read_csv_arrow(input) 
```

-->

## Troubleshooting S3 Connections

Before running an `s3_bucket()` call, it can be helpful to unset the following environmental variables.

```{r}
Sys.setenv("AWS_EC2_METADATA_DISABLED"="TRUE")
Sys.unsetenv("AWS_ACCESS_KEY_ID")
Sys.unsetenv("AWS_SECRET_ACCESS_KEY")
Sys.unsetenv("AWS_DEFAULT_REGION")
Sys.unsetenv("AWS_S3_ENDPOINT")
```


